title: Selecting Knowledge for Microsoft 365 Copilot Tuning
description: "Learn how to prepare and select the right knowledge sources when fine-tuning Microsoft 365 Copilot, including required document types, data limitations, and best practices."
author: your-github-alias
ms.author: your-alias
ms.service: Microsoft 365 Copilot
ms.topic: overview
ms.date: 06/13/2025

#customer intent: As  model maker, I want to fine-tune Microsoft 365 Copilot with my organization's documents so that it produces content and answers aligned with our business context.

# What is Microsoft 365 Copilot Tuning?

Microsoft 365 Copilot Tuning is a **new capability that allows organizations to fine-tune large language models using their own tenant data**[1](https://learn.microsoft.com/en-us/copilot/microsoft-365/copilot-tuning-overview). It lets you create a custom AI assistant (Copilot agent) that behaves like an expert team member, providing tailored assistance in line with your organization's content and rules[1](https://learn.microsoft.com/en-us/copilot/microsoft-365/copilot-tuning-overview). By selecting and feeding *knowledge* from your company’s internal documents into the fine-tuning process, you enable the model to learn your unique terminology, style, and procedures. All training and AI processing occur within your Microsoft 365 tenant, so your data remains secure and governed by existing compliance controls[1](https://learn.microsoft.com/en-us/copilot/microsoft-365/copilot-tuning-overview).

In practice, **selecting knowledge for Copilot Tuning** means identifying and preparing the right set of content from your organization that the model will learn from. This includes gathering representative documents, examples, and instructions that capture the expertise you want the finetuned model to mimic. For example, a legal department could fine-tune a model on the firm's past case briefs and templates, so the Copilot can draft contracts in the firm’s style and terminology[1](https://learn.microsoft.com/en-us/copilot/microsoft-365/copilot-tuning-overview). The better the selected knowledge reflects your domain and task, the more the fine-tuned Copilot will produce relevant, high-quality results.

**In this article,** we introduce how to select and prepare knowledge for Copilot Tuning. You'll learn about the types of documents needed, how to organize them, basic requirements (like minimum data samples and file formats), as well as dependencies, limitations, and overhead to consider. By understanding knowledge selection, you can ensure your fine-tuned Copilot agent is effective and aligned with your needs.

## Preparing the right knowledge for tuning

Selecting knowledge is the first and most critical step in Copilot Tuning. You should **curate a high-quality training dataset** from your most relevant and authoritative content[2](https://www.microsoft.com/en-us/microsoft-365/blog/2025/05/19/introducing-microsoft-365-copilot-tuning-multi-agent-orchestration-and-more-from-microsoft-build-2025/). The key is to provide examples that teach the model exactly what you expect it to do. The content you choose will depend on the task type (Copilot Tuning currently supports three primary task scenarios: *expert Q&A*, *document generation*, and *document summarization*[1](https://learn.microsoft.com/en-us/copilot/microsoft-365/copilot-tuning-overview)). For each scenario, consider the following:

- **Supported file formats and content:** Copilot Tuning supports common text-based document formats. You can use **Word documents (.doc, .docx)**, webpages or HTML files (.html, .aspx), **Markdown files (.md)**, or OCR'd PDFs as source materials. The tuning process will **ingest the text content** from these files (it does *not* learn from images, embedded tables, or other non-text elements in the documents). Ensure that the important information in your training documents is in textual form. For example, if a target report PDF contains a chart, include a textual explanation of that chart’s insights in the document or as supplementary notes, since the model cannot directly learn from the image of the chart. By sticking to supported formats and text content, you make sure the model has access to all the knowledge it needs. (Note that content in unsupported formats or unstructured web content will be ignored by the tuning process.)

- **Number of documents:** You must have at least 20 documents or document examples before training with Copilot Tuning. Usually 100s or 1000s of samples is ideal but quality of samples is often more important the quantity. We recommend you focus you data preparation time on finding as many high quality samples as possible rather than focusing on quantity of samples.

- **Model instructions (metadata about knowledge source contents):** Copilot Tuning, as a part of the model configuration process, asks the model maker to provide answers to a series of **model instructions** to guide the system about how to use the knowledge you have selected. Each task type has its own questions about the selected knowledge source. Prepare a clear, structured answers to each question. For example, Expert Q&A requires a description of the data in the knowledge source and how it is organized, document generation requires you to specify how the original input, changes and output draft document are referred to in your organization, and summarization require you to specify how to refer to the summaries. Just as when you provide an effective prompt to an LLM, it is important that this information is clear and accurately represents your data in order for the system to be most effective.

By carefully collecting these ingredients — the right documents and clear instructions — you are effectively **selecting the “knowledge” that Copilot will learn**. In summary, you’ll provide Copilot Tuning with a *knowledge source* consisting of: your documents and guidance to tie them together. The Copilot Studio interface will guide you to upload or point to this content in your tenant. Once this knowledge selection is done, the service fine-tunes the model on this data entirely within your tenant, producing a specialized model that can reproduce the patterns it learned from your examples.

## Limitations and considerations

While Copilot Tuning is powerful, there are important limitations and considerations to keep in mind when selecting knowledge and fine-tuning:

- **Text-only understanding:** The fine-tuning process **only learns from textual content** in your documents. Any information in images, diagrams, scanned PDFs, or other non-text formats will not be understood by the model. Similarly, complex formatting like detailed tables or embedded spreadsheets may not translate into the model’s training (the text may be read, but the structural meaning could be lost). Make sure any crucial data is expressed in plain text form in the training materials or in the supplementary instructions. For example, instead of expecting the model to learn a procedure from a flowchart image, write out the steps from that flowchart in text. *If it’s not text, Copilot Tuning won’t capture it.*

- **Content scope and model capacity:** Very large documents might be truncated or need to be broken into parts. Underlying models have context length limits when learning patterns. If you have extremely lengthy files (dozens of pages), consider whether all that content is needed for tuning. It may be better to train on multiple smaller, focused documents than one monolithic file. Also ensure your examples concentrate on the relevant portions to teach the target task. Irrelevant or extraneous text in training data can confuse the model. At the same time, avoid training data that is too brief or insufficient.

- **Static snapshot of knowledge:** The fine-tuned model represents a snapshot of the knowledge at the time of training. **It will not automatically update** if your source documents change or new documents are added. For instance, if you fine-tuned a model on a policy manual and that manual gets revised next quarter, the model will still reflect the old policy until you retrain it with the new information. This is different from the standard Copilot behavior which uses Retrieval-Augmented Generation (searching live data at query time). Fine-tuning trades real-time updating for deeper learned expertise. You should plan to retrain (or at least evaluate) the model periodically or when significant changes in your domain occur. *Important:* Changes to document permissions after training do not affect the model immediately[1](https://learn.microsoft.com/en-us/copilot/microsoft-365/copilot-tuning-overview). If someone loses access to a source file after the model was trained, the model might still include knowledge from that file. As an admin, you may need to regulate model access or retrain if necessary to comply with any evolving access policies.

By understanding these limitations, you can better plan your knowledge selection and set correct expectations for the tuned Copilot. Mitigate the limitations by providing good data and maintaining the model over time. For instance, to address the static knowledge issue, schedule regular reviews of the agent’s output against current documents and decide if it needs retraining every so often.

## Overhead and maintenance

Implementing Copilot Tuning does introduce some overhead in terms of effort and ongoing maintenance, but this is manageable with proper planning:

- **Initial effort to prepare data:** The most labor-intensive part is **collecting and organizing the training knowledge**. You’ll spend time finding suitable documents, cleaning or annotating them (for example, removing any sensitive sections that shouldn’t be in the training, or writing the structured change instructions as discussed). You may also need to coordinate with colleagues (for example, domain experts who know which documents are the best examples). This upfront effort is crucial—better preparation leads to far less frustration later. Treat it as an investment in creating your “expert dataset.” Many teams find that in doing this, they also end up improving their document management (e.g., identifying canonical templates, consolidating reference materials), which is a side benefit.

- **Iteration and evaluation:** Fine-tuning is often **an iterative process**[2](https://www.microsoft.com/en-us/microsoft-365/blog/2025/05/19/introducing-microsoft-365-copilot-tuning-multi-agent-orchestration-and-more-from-microsoft-build-2025/). The first model version may not be perfect. Plan for iteration: after the model is trained, spend time testing it with realistic prompts. Have subject matter experts evaluate the outputs. You might discover that the model is weak on a certain subtopic or format. In that case, you’ll want to go back and add a few more training examples or refine your instructions, then retrain. Build in time for at least one or two refinement cycles. Each cycle means a bit more data prep and another training run.


## Related content

- [Microsoft 365 Copilot Tuning overview (preview)](https://learn.microsoft.com/en-us/copilot/microsoft-365/copilot-tuning-overview) – High-level documentation of Copilot Tuning, its scenarios, and best practices for fine-tuning models with your data.
- https://learn.microsoft.com/en-us/microsoft-365/copilot/copilot-overview – Overview of Microsoft 365 Copilot, the AI assistant for work, providing context on how Copilot operates and the base capabilities prior to any tuning.
- [Introducing Microsoft 365 Copilot Tuning (Microsoft 365 Blog)](https://www.microsoft.com/en-us/microsoft-365/blog/2025/05/19/introducing-microsoft-365-copilot-tuning-multi-agent-orchestration-and-more-from-microsoft-build-2025/) – Official blog announcement of Copilot Tuning and related features (Build 2025 news), useful for understanding Microsoft's vision and use cases for fine-tuned Copilots.