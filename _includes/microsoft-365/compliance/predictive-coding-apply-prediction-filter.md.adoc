= Apply the prediction score filter to a review set
:audience: Admin
:author: robmazz
:description: Use a prediction score filter to displays items that a predictive coding model as predicted as relevant or not relevant.
:f1.keywords: ["NOCSH"]
:manager: laurawi
:ms.author: robmazz
:ms.collection: ["tier1", "M365-security-compliance", "ediscovery"]
:ms.localizationpriority: medium
:ms.reviewer: jefwan
:ms.service: O365-seccomp
:ms.topic: article
:search.appverid: ["MET150"]

== Apply a prediction score filter to a review set (preview)

After you create a predictive coding model in Microsoft Purview eDiscovery (Premium) and train it to the point where it's stable, you can apply the prediction score filter to display review set items that the model has determined are relevant (or not relevant).
When you create a model, a corresponding prediction score filter is also created.
You can use this filter to display items assigned a prediction score within a specified range.
In general, prediction scores between *0* and *.5* are assigned to items that model has predicted are not relevant.
Items assigned prediction scores between *.5* and *1.0* are items the model has predicted are relevant.

Here are two ways you can use the prediction score filter:

* Prioritize the review of items in a review set that the model has predicted are relevant.
* Cull items from the review set that the model has predicted are not relevant.
Alternative, you can use the prediction score filter to de-prioritize the review of non-relevant items.

=== Before you apply a prediction score filter

* Create a predictive coding model so that a corresponding prediction score filter is created.
* You can apply a prediction score filter after any of the training rounds.
But you may want to wait after performing several rounds or until the model is stable before using the prediction score filter.

=== Apply a prediction score filter

. In the Microsoft Purview compliance portal, open the eDiscovery (Premium) case, select the *Review sets* tab, and then open the review set.
+
image::..\media\PredictionScoreFilter0.png[Click Filters to display the Filters flyout page.]
+
The pre-loaded default filters are displayed at the top of the review set page.
You can leave these set to *Any*.

. Click *Filters* to display the *Filters* flyout page.
. Expand the *Analytics & predictive coding* section to display a set of filters.
+
image::..\media\PredictionScoreFilter1.png[Prediction score filter in the Analytics & predictive coding section.]
+
The naming convention for prediction score filters is *Prediction score (model name)*.
For example, the prediction score filter name for a model named *Model A* is *Prediction score (Model A)*.

. Select the prediction score filter that you want to use and then click *Done*.
. On the review set page, click the dropdown for the prediction score filter and type minimum and maximum values for the prediction score range.
For example, the following screenshot shows a prediction score range between *.5* and *1.0*.
+
image::..\media\PredictionScoreFilter2.png[Minimum and maximum values for the prediction score filter.]

. Click outside the filter to automatically apply the filter to the review set.

A list of documents with a prediction score within the range you specified is displayed on the review set page.

____
[!TIP] To view the actual prediction score assign to a document, you can click the *Metadata* tab in the reading pane.
The prediction scores for all models in the review set are displayed in the *RelevanceScores* metadata property.
____

=== More information

* For more information about using filters, see xref:review-set-search.adoc[Query and filter content in a review set].
