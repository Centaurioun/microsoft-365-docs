= Test Relevance analysis in eDiscovery (Premium)
:ROBOTS: NOINDEX, NOFOLLOW
:audience: Admin
:author: robmazz
:description: Learn how to use the Test tab after Batch calculation in eDiscovery (Premium) to test, compare, and validate the overall quality of processing.
:experimental:
:f1.keywords: ["NOCSH"]
:manager: laurawi
:ms.author: robmazz
:ms.collection: ["tier1", "M365-security-compliance", "ediscovery"]
:ms.localizationpriority: medium
:ms.service: O365-seccomp
:ms.topic: article
:search.appverid: ["MOE150", "MET150"]
:titleSuffix: Office 365

== Test Relevance analysis in eDiscovery (Premium)

The Test tab in Microsoft Purview eDiscovery (Premium) enables you to test, compare, and validate the overall quality of processing.
These tests are performed after Batch calculation.
By tagging the files in the collection, an expert makes the final judgment about whether each tagged file is relevant to the case.

In single and multiple-issue scenarios, tests are typically performed per issue.
Results can be viewed after each test, and test results can be reworked with specified sample test files.

=== Testing the rest

The "Test the Rest" test is used to validate culling decisions, for example, to review only files above a specific Relevance cutoff score based on the final eDiscovery (Premium) results.
The expert reviews a sample of files under a selected cutoff score to evaluate the number of relevant files within that set.

This test provides statistics and a comparison between the Review set and the Test the Rest population.
The results of the review set are those calculated by Relevance during Training.
The results include calculations based on settings and input parameters, such as:

* Test sample statistics of the number of files in a sample and identified relevant files.
* Tabular comparison of the Population parameters of the Review set and the Rest, for example, the number of files, estimated number of relevant files, estimated richness, and the average cost of finding another relevant file.
Cost parameter settings can be set by the administrator.

To run the "Test the Rest" test:

. Open the menu:Relevance[Test] tab.
. In the *Test* tab, click *New test*.
The *Create test* dialog is displayed, as shown in the following example.
+
image::../media/46e6898a-f929-4fd0-88d9-6f91d04b6ce2.png[Relevance Test the Rest results.]

. In *Test name*, and *Description*, type the name and description.
. In the *Test type* list, select *Test the Rest*
. In the *Issue / Category* list, select the issue name.
. In the *Load* list, select the load.
. In *Read %*, accept the default value or select a value for the cutoff Relevance score.
. In *Set size*, or accept the default value.
The restore icons will restore the default values.
. Click *Start tagging*.
A test sample is generated.
. Review and tag each of the files in the menu:Relevance[Tag] tab and when done, click *Calculate*.
. In the Test tab, you can click *View results* to see the test results.
An example is shown in the following screenshot.
+
image::../media/b95744a9-047d-4c29-992d-04fa7e58e58a.png[Test the rest results.]

In the previous screenshot, the *Sample parameters* section of the table contains details about the number of files in the sample tagged by the expert, and the number of relevant files found in that sample.

The *Population parameters* section of the table contains the test results, including the Review set population of files with a score below the selected cutoff and "The Rest" population of files with a score above the selected cutoff.
For each population, the following results are displayed:

* Includes files with read % - Stated cutoff
* The total number of files
* The estimated number of relevant files
* The estimated richness
* The average review cost of finding another relevant file

=== Testing the slice

The "Test the Slice" test performs testing similar to the "Test the Rest" test, but to a segment of the file set as specified by Relevance Read %.

To run the "Test the Slice" test:

. Open the menu:Relevance[Test] tab.
. In the *Test* tab, click *New test*.
The *Create test* dialog is displayed.
. In *Test name* and *Description*, type the information.
. In the *Test type* list, select *Test the Slice*.
. In the *Issue* list, select the issue name.
. In the *Load* list, select the load.
. In *Read % between*, accept the default low and high range values or select values for the cutoff Relevance scores.
. In *Set size*, select a value or accept the default value.
+
The restore icons will restore the default value.

. Click *Start tagging*.
A test sample is generated.
. Review and tag each of the files in the menu:Relevance[Tag] tab and when done, click *Calculate*.
. In the Test tab, you can click *View results* to see the test results.
